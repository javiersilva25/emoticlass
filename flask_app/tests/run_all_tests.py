#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Script Maestro de Testing - Sistema de AnÃ¡lisis de Emociones
Lemac Datalab - Suite de Testing Riguroso Completo
"""

import sys
import os
import time
import subprocess
import json
from datetime import datetime

# Agregar el directorio padre al path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Importar mÃ³dulos de testing
from test_unit import run_unit_tests
from test_integration import run_integration_tests
from test_e2e import run_e2e_tests
from test_performance import run_performance_tests

class TestRunner:
    """Ejecutor maestro de todos los tests"""
    
    def __init__(self):
        self.results = {
            'start_time': None,
            'end_time': None,
            'duration': None,
            'unit_tests': {'executed': False, 'passed': False, 'details': None},
            'integration_tests': {'executed': False, 'passed': False, 'details': None},
            'e2e_tests': {'executed': False, 'passed': False, 'details': None},
            'performance_tests': {'executed': False, 'passed': False, 'details': None},
            'overall_success': False,
            'summary': {}
        }
    
    def print_header(self):
        """Imprimir header del testing"""
        print("=" * 80)
        print("ğŸ§ª SISTEMA DE TESTING RIGUROSO - LEMAC DATALAB")
        print("ğŸ“Š Sistema de AnÃ¡lisis de Emociones")
        print("=" * 80)
        print(f"ğŸ• Inicio: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 80)
    
    def print_section_header(self, section_name, description):
        """Imprimir header de secciÃ³n"""
        print(f"\n{'='*20} {section_name} {'='*20}")
        print(f"ğŸ“‹ {description}")
        print("-" * 60)
    
    def run_unit_tests_section(self):
        """Ejecutar tests unitarios"""
        self.print_section_header("TESTS UNITARIOS", "ValidaciÃ³n de funciones individuales")
        
        try:
            start_time = time.time()
            success = run_unit_tests()
            end_time = time.time()
            
            self.results['unit_tests'] = {
                'executed': True,
                'passed': success,
                'duration': end_time - start_time,
                'details': "Tests unitarios completados"
            }
            
            if success:
                print("âœ… TESTS UNITARIOS: EXITOSOS")
            else:
                print("âŒ TESTS UNITARIOS: FALLARON")
                
        except Exception as e:
            print(f"ğŸ’¥ ERROR EN TESTS UNITARIOS: {e}")
            self.results['unit_tests'] = {
                'executed': True,
                'passed': False,
                'error': str(e)
            }
    
    def run_integration_tests_section(self):
        """Ejecutar tests de integraciÃ³n"""
        self.print_section_header("TESTS DE INTEGRACIÃ“N", "ValidaciÃ³n de APIs y rutas Flask")
        
        try:
            start_time = time.time()
            success = run_integration_tests()
            end_time = time.time()
            
            self.results['integration_tests'] = {
                'executed': True,
                'passed': success,
                'duration': end_time - start_time,
                'details': "Tests de integraciÃ³n completados"
            }
            
            if success:
                print("âœ… TESTS DE INTEGRACIÃ“N: EXITOSOS")
            else:
                print("âŒ TESTS DE INTEGRACIÃ“N: FALLARON")
                
        except Exception as e:
            print(f"ğŸ’¥ ERROR EN TESTS DE INTEGRACIÃ“N: {e}")
            self.results['integration_tests'] = {
                'executed': True,
                'passed': False,
                'error': str(e)
            }
    
    def run_e2e_tests_section(self):
        """Ejecutar tests end-to-end"""
        self.print_section_header("TESTS END-TO-END", "ValidaciÃ³n de flujos completos de usuario")
        
        try:
            start_time = time.time()
            success = run_e2e_tests()
            end_time = time.time()
            
            self.results['e2e_tests'] = {
                'executed': True,
                'passed': success,
                'duration': end_time - start_time,
                'details': "Tests end-to-end completados"
            }
            
            if success:
                print("âœ… TESTS END-TO-END: EXITOSOS")
            else:
                print("âŒ TESTS END-TO-END: FALLARON")
                
        except Exception as e:
            print(f"ğŸ’¥ ERROR EN TESTS END-TO-END: {e}")
            self.results['e2e_tests'] = {
                'executed': True,
                'passed': False,
                'error': str(e)
            }
    
    def run_performance_tests_section(self):
        """Ejecutar tests de rendimiento"""
        self.print_section_header("TESTS DE RENDIMIENTO", "ValidaciÃ³n de performance y carga")
        
        try:
            start_time = time.time()
            success = run_performance_tests()
            end_time = time.time()
            
            self.results['performance_tests'] = {
                'executed': True,
                'passed': success,
                'duration': end_time - start_time,
                'details': "Tests de rendimiento completados"
            }
            
            if success:
                print("âœ… TESTS DE RENDIMIENTO: EXITOSOS")
            else:
                print("âŒ TESTS DE RENDIMIENTO: FALLARON")
                
        except Exception as e:
            print(f"ğŸ’¥ ERROR EN TESTS DE RENDIMIENTO: {e}")
            self.results['performance_tests'] = {
                'executed': True,
                'passed': False,
                'error': str(e)
            }
    
    def check_dependencies(self):
        """Verificar dependencias necesarias para testing"""
        print("ğŸ” Verificando dependencias de testing...")
        
        dependencies = {
            'requests': 'pip install requests',
            'psutil': 'pip install psutil',
            'selenium': 'pip install selenium'
        }
        
        missing_deps = []
        
        for dep, install_cmd in dependencies.items():
            try:
                __import__(dep)
                print(f"âœ… {dep}: Disponible")
            except ImportError:
                print(f"âŒ {dep}: No disponible - {install_cmd}")
                missing_deps.append(dep)
        
        if missing_deps:
            print(f"\nâš ï¸ Dependencias faltantes: {', '.join(missing_deps)}")
            print("ğŸ’¡ Algunos tests pueden ser omitidos")
        else:
            print("âœ… Todas las dependencias estÃ¡n disponibles")
        
        return len(missing_deps) == 0
    
    def generate_summary(self):
        """Generar resumen de resultados"""
        total_executed = sum(1 for test in self.results.values() 
                           if isinstance(test, dict) and test.get('executed', False))
        total_passed = sum(1 for test in self.results.values() 
                         if isinstance(test, dict) and test.get('passed', False))
        
        self.results['summary'] = {
            'total_test_suites': 4,
            'executed_test_suites': total_executed,
            'passed_test_suites': total_passed,
            'success_rate': (total_passed / total_executed * 100) if total_executed > 0 else 0
        }
        
        self.results['overall_success'] = total_passed == total_executed and total_executed > 0
    
    def print_final_report(self):
        """Imprimir reporte final"""
        print("\n" + "=" * 80)
        print("ğŸ“Š REPORTE FINAL DE TESTING")
        print("=" * 80)
        
        # Resumen por tipo de test
        test_types = [
            ('unit_tests', 'Tests Unitarios', 'ğŸ§ª'),
            ('integration_tests', 'Tests de IntegraciÃ³n', 'ğŸ”—'),
            ('e2e_tests', 'Tests End-to-End', 'ğŸ­'),
            ('performance_tests', 'Tests de Rendimiento', 'ğŸš€')
        ]
        
        for test_key, test_name, emoji in test_types:
            test_result = self.results[test_key]
            if test_result['executed']:
                status = "âœ… EXITOSO" if test_result['passed'] else "âŒ FALLIDO"
                duration = test_result.get('duration', 0)
                print(f"{emoji} {test_name:25} {status:12} ({duration:.2f}s)")
            else:
                print(f"{emoji} {test_name:25} â­ï¸ OMITIDO")
        
        print("-" * 80)
        
        # EstadÃ­sticas generales
        summary = self.results['summary']
        print(f"ğŸ“ˆ ESTADÃSTICAS GENERALES:")
        print(f"   Suites de test ejecutadas: {summary['executed_test_suites']}/{summary['total_test_suites']}")
        print(f"   Suites exitosas: {summary['passed_test_suites']}/{summary['executed_test_suites']}")
        print(f"   Tasa de Ã©xito: {summary['success_rate']:.1f}%")
        print(f"   DuraciÃ³n total: {self.results['duration']:.2f}s")
        
        print("-" * 80)
        
        # Resultado final
        if self.results['overall_success']:
            print("ğŸ‰ RESULTADO FINAL: TODOS LOS TESTS PASARON EXITOSAMENTE")
            print("âœ… EL SISTEMA ESTÃ VALIDADO Y LISTO PARA PRODUCCIÃ“N")
        else:
            print("ğŸ’¥ RESULTADO FINAL: ALGUNOS TESTS FALLARON")
            print("âš ï¸ REVISAR ERRORES ANTES DE DESPLEGAR EN PRODUCCIÃ“N")
        
        print("=" * 80)
    
    def save_results_to_file(self):
        """Guardar resultados en archivo JSON"""
        try:
            results_file = os.path.join(os.path.dirname(__file__), 'test_results.json')
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(self.results, f, indent=2, ensure_ascii=False, default=str)
            print(f"ğŸ“„ Resultados guardados en: {results_file}")
        except Exception as e:
            print(f"âš ï¸ No se pudieron guardar los resultados: {e}")
    
    def run_all_tests(self, skip_e2e=False, skip_performance=False):
        """Ejecutar todos los tests"""
        self.results['start_time'] = datetime.now()
        start_time = time.time()
        
        self.print_header()
        
        # Verificar dependencias
        deps_ok = self.check_dependencies()
        
        # Ejecutar cada suite de tests
        self.run_unit_tests_section()
        self.run_integration_tests_section()
        
        if not skip_e2e:
            self.run_e2e_tests_section()
        else:
            print("\nâ­ï¸ TESTS END-TO-END OMITIDOS (skip_e2e=True)")
        
        if not skip_performance:
            self.run_performance_tests_section()
        else:
            print("\nâ­ï¸ TESTS DE RENDIMIENTO OMITIDOS (skip_performance=True)")
        
        # Calcular duraciÃ³n total
        end_time = time.time()
        self.results['end_time'] = datetime.now()
        self.results['duration'] = end_time - start_time
        
        # Generar resumen y reporte final
        self.generate_summary()
        self.print_final_report()
        self.save_results_to_file()
        
        return self.results['overall_success']

def main():
    """FunciÃ³n principal"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Ejecutar suite completa de tests')
    parser.add_argument('--skip-e2e', action='store_true', 
                       help='Omitir tests end-to-end (requieren Selenium)')
    parser.add_argument('--skip-performance', action='store_true',
                       help='Omitir tests de rendimiento (pueden ser lentos)')
    parser.add_argument('--unit-only', action='store_true',
                       help='Ejecutar solo tests unitarios')
    parser.add_argument('--integration-only', action='store_true',
                       help='Ejecutar solo tests de integraciÃ³n')
    
    args = parser.parse_args()
    
    runner = TestRunner()
    
    if args.unit_only:
        runner.print_header()
        runner.run_unit_tests_section()
        success = runner.results['unit_tests']['passed']
    elif args.integration_only:
        runner.print_header()
        runner.run_integration_tests_section()
        success = runner.results['integration_tests']['passed']
    else:
        success = runner.run_all_tests(
            skip_e2e=args.skip_e2e,
            skip_performance=args.skip_performance
        )
    
    # CÃ³digo de salida
    sys.exit(0 if success else 1)

if __name__ == '__main__':
    main()

